import numpy as np
import matplotlib.pyplot as plt



def get_dynamic_probabilities(step):
    # Base means for the bandits
    probs = [
        np.random.normal(60, 7),  # Reading
        np.random.normal(75, 10),  # Audio
        np.random.normal(95, 5)   # Video
    ]
    
    # Apply drift to each bandit's mean
    drift = 0.001 * step
    probs = [min(p + drift, 100) for p in probs]  # Add drift and cap at 100
    
    # Clip to ensure values stay within 0â€“100
    probs = [max(0, p) for p in probs]
    
    return probs

# Modified Thompson Sampling algorithm with dynamic bandits
def thompson_sampling_dynamic(num_steps=1000, num_actions=3):
    # Initialize beta distributions for each action
    alpha = np.ones(num_actions)
    beta = np.ones(num_actions)
    rewards = np.zeros(num_steps)
    
    for step in range(num_steps):
        # Sample from the beta distribution for each action
        theta_samples = np.random.beta(alpha, beta)
        action = np.argmax(theta_samples)  # Choose the action with the highest sample
        
        # Get the dynamic probabilities for the current step
        probabilities = get_dynamic_probabilities(step)
        
        reward = np.random.normal(probabilities[action], 1)
        normalized_reward = max(0, (reward - 50) / 50)
        
        # Update beta distributions based on the reward
        if normalized_reward > 0:
            alpha[action] += 1
        else:
            beta[action] += 1
        
        rewards[step] = reward
    
    # Return cumulative rewards over time
    return np.cumsum(rewards) / (np.arange(num_steps) + 1)

average_rewards_dynamic = {}
    
avg_reward_tsd = thompson_sampling_dynamic()

# Plotting all epsilon-greedy_dynamic results
plt.figure(figsize=(12, 8))
plt.plot(avg_reward_tsd, label='Thompson Dynamic Sampling', linestyle='--', color='red')
plt.xlabel('Steps')
plt.ylabel('Average Reward')
plt.legend()
plt.title('Thompson Dynamic Sampling: Average Reward vs. Steps')
plt.show()

