using python3

install numpy
install matplotlib

run 
python3 final.py

### Technical Summary of the Explore/Exploit Algorithm

#### **Objective**  
This algorithm applies **Thompson Sampling (TS)** to identify the preferred learning method (reading, audio, or video) in a simulated dynamic environment. The environment models changing preferences over time through a drift in reward probabilities.

---

#### **Algorithm Description**

1. **Dynamic Environment Setup**  
   - Reward probabilities for each learning method are defined with base means:
     - **Reading:** \( \mathcal{N}(60, 7) \)
     - **Audio:** \( \mathcal{N}(75, 10) \)
     - **Video:** \( \mathcal{N}(95, 5) \)
   - A **drift** factor (\( 0.001 \times \text{step} \)) is added to simulate non-stationary preferences, capped at 100.

2. **Reward Normalization**  
   - Rewards are sampled from a normal distribution around the current mean of the chosen method.
   - Normalization ensures consistent scaling:
     \[
     \text{normalized\_reward} = \max\left(0, \frac{\text{reward} - 50}{50}\right)
     \]

3. **Thompson Sampling Mechanism**  
   - Each method is modeled using a Beta distribution parameterized by \(\alpha\) (successes) and \(\beta\) (failures).  
   - At each step:
     - **Sampling:** Draw samples from Beta distributions (\(\text{Beta}(\alpha_i, \beta_i)\)).
     - **Action Selection:** Choose the action with the highest sample (\(\theta_i\)).
   - Update the parameters based on observed rewards:
     - Increment \(\alpha[action]\) if \( \text{normalized\_reward} > 0 \).
     - Increment \(\beta[action]\) otherwise.

4. **Performance Tracking**  
   - Cumulative rewards are recorded and averaged to track the algorithmâ€™s performance over time.

---

#### **Results and Insights**

1. **Plot Overview**  
   - The chart displays average reward over time. Early fluctuations are caused by limited data and exploratory actions, while later stability reflects successful adaptation to the environment.

2. **Key Insights**  
   - **Adaptivity:** The algorithm dynamically balances exploration and exploitation using Bayesian updates, adapting to changes in reward probabilities over time.
   - **Performance:** Despite early-stage variability, the algorithm converges towards maximizing average rewards in a dynamic, non-stationary environment.

---

#### **Applications**  
This implementation is well-suited for dynamic optimization problems, such as personalizing learning methods in educational apps where user preferences evolve over time.
